{"cells":[{"cell_type":"markdown","id":"5e5aee91","metadata":{"cell_marker":"\"\"\"","id":"5e5aee91"},"source":["# LLM API Integration: From Basics to Advanced Applications\n","\n","This notebook provides a comprehensive introduction to using Large Language Models (LLMs) via APIs in your applications. Before diving into complex agent-based systems, it's important to understand the fundamentals of API interaction with modern language models.\n","\n","## Learning Objectives\n","\n","1. Understand basic API calling patterns for OpenAI and other LLM providers\n","2. Learn about key parameters that control model behavior\n","3. Explore strategies for managing context length and efficient batching\n","4. Implement best practices for caching and cost optimization\n","5. Utilize advanced capabilities including tool calls and multimodal inputs\n","6. Apply these concepts to real-world applications\n","\n","## Prerequisites\n","\n","- Basic Python knowledge\n","- OpenAI API key (and optionally keys for other providers)\n","- Understanding of how to run a Jupyter notebook"]},{"cell_type":"markdown","id":"a76ee879","metadata":{"cell_marker":"\"\"\"","id":"a76ee879"},"source":["## 1 - Environment Setup\n","\n","First, let's set up our environment with the necessary packages. We'll be using:\n","- `openai`: For accessing OpenAI's models\n","- `anthropic`: (Optional) For Claude models\n","- `requests`: For making HTTP requests to other API providers\n","- `tiktoken`: For token counting\n","- `matplotlib`: For visualizations\n","- `pandas`: For data handling"]},{"cell_type":"code","execution_count":null,"id":"b72b42f2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b72b42f2","executionInfo":{"status":"ok","timestamp":1752774785310,"user_tz":240,"elapsed":4536,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"032cf9c2-6feb-4b33-8248-a0d3c6ec192a"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ openai 1.96.1\n","‚úÖ tiktoken 0.9.0\n","‚úÖ matplotlib\n","‚úÖ pandas 2.2.2\n"]}],"source":["import os\n","import getpass\n","import json\n","import time\n","from typing import List, Dict, Any, Optional, Union\n","\n","# Install necessary packages if not already installed\n","try:\n","    import openai\n","    print(f\"‚úÖ openai {openai.__version__}\")\n","except ImportError:\n","    print(\"‚è≥ Installing openai...\")\n","    !pip install -q openai\n","    import openai\n","    print(f\"‚úÖ openai {openai.__version__}\")\n","\n","try:\n","    import tiktoken\n","    print(f\"‚úÖ tiktoken {tiktoken.__version__}\")\n","except ImportError:\n","    print(\"‚è≥ Installing tiktoken...\")\n","    !pip install -q tiktoken\n","    import tiktoken\n","    print(f\"‚úÖ tiktoken {tiktoken.__version__}\")\n","\n","try:\n","    import matplotlib.pyplot as plt\n","    print(\"‚úÖ matplotlib\")\n","except ImportError:\n","    print(\"‚è≥ Installing matplotlib...\")\n","    !pip install -q matplotlib\n","    import matplotlib.pyplot as plt\n","    print(\"‚úÖ matplotlib\")\n","\n","try:\n","    import pandas as pd\n","    print(f\"‚úÖ pandas {pd.__version__}\")\n","except ImportError:\n","    print(\"‚è≥ Installing pandas...\")\n","    !pip install -q pandas\n","    import pandas as pd\n","    print(f\"‚úÖ pandas {pd.__version__}\")"]},{"cell_type":"markdown","id":"a31be3aa","metadata":{"id":"a31be3aa"},"source":["# Note: Setting Up Virtual Environments\n","\n","To isolate dependencies for a project, you can create a virtual environment. You can create one by running `python -m venv env` in your terminal, which will create a folder named `env` in your current directory.\n","\n","Activate the environment with `.\\env\\Scripts\\activate` on Windows or `source env/bin/activate` on macOS/Linux. Once activated, your terminal prompt will change to show the environment name. Install the required packages with `pip install openai tiktoken matplotlib pandas`. When you're finished working, you can deactivate the environment by simply typing `deactivate`.\n"]},{"cell_type":"markdown","id":"083f4adf","metadata":{"cell_marker":"\"\"\"","id":"083f4adf"},"source":["## 2 - API Key Management\n","\n","Most LLM providers require authentication through API keys. Let's set up our API keys safely:"]},{"cell_type":"code","execution_count":null,"id":"55ffcb5f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55ffcb5f","executionInfo":{"status":"ok","timestamp":1752774859668,"user_tz":240,"elapsed":5470,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"5a16f5d1-70f2-4c67-aeeb-99e2daa9bf99"},"outputs":[{"name":"stdout","output_type":"stream","text":["üîë Enter your OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"]}],"source":["# Setting up OpenAI API key\n","if not os.getenv(\"OPENAI_API_KEY\"):\n","    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"üîë Enter your OpenAI API key: \")\n","\n","# Initialize clients\n","from openai import OpenAI\n","client = OpenAI()  # Uses the OPENAI_API_KEY environment variable by default"]},{"cell_type":"markdown","id":"90633bac","metadata":{"cell_marker":"\"\"\"","id":"90633bac"},"source":["### API Key Security Best Practices\n","\n","When working with API keys, always follow these best practices:\n","\n","1. **Never hardcode API keys** in your source code\n","2. **Don't commit API keys** to version control\n","3. Use **environment variables** for production\n","4. Set up **usage limits** in your API provider dashboard\n","5. **Rotate keys periodically** and when team members change\n","\n","For local development, you can use `.env` files, but remember to add them to `.gitignore`."]},{"cell_type":"markdown","id":"16d7544f","metadata":{"cell_marker":"\"\"\"","lines_to_next_cell":1,"id":"16d7544f"},"source":["## 3 - Basic API Calling Patterns\n","\n","### OpenAI Completion API\n","\n","The most fundamental interaction with LLMs is the completion API, which generates text based on a prompt. Here's the basic syntax for OpenAI:"]},{"cell_type":"code","execution_count":null,"id":"ddbd807b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddbd807b","executionInfo":{"status":"ok","timestamp":1752775224918,"user_tz":240,"elapsed":1665,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"cf4a3015-fa55-454b-b2ed-ad77e498a6f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Quantum computing harnesses quantum mechanics to process information using qubits, which can represent both 0 and 1 simultaneously. This enables parallel computations and complex problem-solving far beyond classical computers, potentially revolutionizing fields like cryptography, optimization, and drug discovery by offering exponential speed-ups for certain tasks.\n"]}],"source":["def basic_completion(prompt: str, model: str = \"gpt-4o\", temperature: float = 0.7) -> str:\n","    \"\"\"Basic text completion using OpenAI's chat completion API.\"\"\"\n","    try:\n","        response = client.chat.completions.create(\n","            model=model,\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            temperature=temperature\n","        )\n","        return response.choices[0].message.content\n","    except Exception as e:\n","        return f\"Error: {str(e)}\"\n","\n","# Example usage\n","prompt = \"Explain quantum computing in 50 words or less.\"\n","result = basic_completion(prompt)\n","print(result)"]},{"cell_type":"markdown","id":"435c70ee","metadata":{"cell_marker":"\"\"\"","lines_to_next_cell":1,"id":"435c70ee"},"source":["### ChatGPT-style Conversation API\n","\n","Most modern LLMs support conversation-style interactions with different roles. Here's how to implement a conversation:"]},{"cell_type":"code","execution_count":null,"id":"d2b8a705","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2b8a705","executionInfo":{"status":"ok","timestamp":1752775301584,"user_tz":240,"elapsed":2197,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"06a80e34-8968-42cc-8735-73951cd97e5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["The Higgs boson was discovered in 2012 at the Large Hadron Collider (LHC) at CERN. Scientists at CERN conducted experiments colliding protons at very high energies and analyzing the data to search for the signature of the Higgs boson. The discovery was confirmed by two independent experiments, ATLAS and CMS, which both observed a particle consistent with the predicted properties of the Higgs boson.\n"]}],"source":["def chat_conversation(messages: List[Dict[str, str]],\n","                      model: str = \"gpt-3.5-turbo\",\n","                      temperature: float = 0.7) -> str:\n","    \"\"\"Multi-turn conversation using OpenAI's chat completion API.\n","       This is designed to send a complete conversation history to the API and get the next response.\"\"\"\n","    try:\n","        response = client.chat.completions.create(\n","            model=model,\n","            messages=messages,\n","            temperature=temperature\n","        )\n","        return response.choices[0].message.content\n","    except Exception as e:\n","        return f\"Error: {str(e)}\"\n","\n","# Example conversation\n","conversation = [\n","    {\"role\": \"system\", \"content\": \"You are a helpful assistant specializing in physics.\"},\n","    {\"role\": \"user\", \"content\": \"What is the Higgs boson?\"},\n","    {\"role\": \"assistant\", \"content\": \"The Higgs boson is an elementary particle in the Standard Model of physics that gives mass to other particles through the Higgs mechanism.\"},\n","    {\"role\": \"user\", \"content\": \"How was it discovered?\"}\n","]\n","\n","result = chat_conversation(conversation)\n","print(result)"]},{"cell_type":"markdown","id":"c49d4ab6","metadata":{"id":"c49d4ab6"},"source":["##### Conversation Structure: The chat API allows you to send multiple messages with different roles:\n","- system: Sets overall behavior/personality instructions\n","- user: Messages from the human\n","- assistant: Previous AI responses"]},{"cell_type":"markdown","id":"34a32218","metadata":{"cell_marker":"\"\"\"","lines_to_next_cell":1,"id":"34a32218"},"source":["### Structured Output with JSON Mode\n","\n","When you need the model to return structured data that can be directly parsed by your application:"]},{"cell_type":"code","execution_count":null,"id":"e56694dc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e56694dc","executionInfo":{"status":"ok","timestamp":1752775445690,"user_tz":240,"elapsed":1573,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"c94623f9-9930-4231-becc-ea1670b5a685"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"product\": {\n","    \"name\": \"Laptop\",\n","    \"price\": \"$1000\",\n","    \"category\": \"Electronics\",\n","    \"features\": {\n","      \"screenSize\": \"15.6 inch\",\n","      \"memory\": \"8GB RAM\",\n","      \"storage\": \"256GB SSD\"\n","    }\n","  }\n","}\n"]}],"source":["def json_output(prompt: str, model: str = \"gpt-3.5-turbo\") -> Dict[str, Any]:\n","    \"\"\"Get structured JSON output from the model.\"\"\"\n","    try:\n","        response = client.chat.completions.create(\n","            model=model,\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            response_format={\"type\": \"json_object\"}\n","        )\n","        return json.loads(response.choices[0].message.content)\n","    except Exception as e:\n","        return {\"error\": str(e)}\n","\n","# Example\n","product_prompt = \"Create a JSON object for a product with name, price, category, and three features.\"\n","product_data = json_output(product_prompt)\n","print(json.dumps(product_data, indent=2))"]},{"cell_type":"markdown","id":"c6ed8644","metadata":{"cell_marker":"\"\"\"","id":"c6ed8644"},"source":["### API Calling Patterns with Other Providers\n","\n","Other LLM providers follow similar patterns, though with some syntax differences:\n","\n","#### Anthropic (Claude)\n","```python\n","import anthropic\n","client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n","\n","response = client.messages.create(\n","    model=\"claude-3-opus-20240229\",\n","    max_tokens=1000,\n","    messages=[\n","        {\"role\": \"user\", \"content\": \"Explain quantum entanglement briefly.\"}\n","    ]\n",")\n","print(response.content[0].text)\n","```\n","\n","#### DeepSeek\n","```python\n","import requests\n","\n","headers = {\n","    \"Authorization\": f\"Bearer {os.environ.get('DEEPSEEK_API_KEY')}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","data = {\n","    \"model\": \"deepseek-chat\",\n","    \"messages\": [\n","        {\"role\": \"user\", \"content\": \"What are transformer models?\"}\n","    ],\n","    \"temperature\": 0.7\n","}\n","\n","response = requests.post(\"https://api.deepseek.com/v1/chat/completions\",\n","                         headers=headers,\n","                         json=data)\n","result = response.json()\n","print(result[\"choices\"][0][\"message\"][\"content\"])\n","```\n","\n","These examples illustrate that while the specific syntax varies, the core pattern of sending a request with messages/prompts and receiving generated text in response remains consistent across providers."]},{"cell_type":"markdown","id":"86f6d780","metadata":{"cell_marker":"\"\"\"","lines_to_next_cell":1,"id":"86f6d780"},"source":["## 4 - Model Parameters and Their Impact\n","\n","Model parameters significantly affect the output quality, creativity, and behavior of LLMs. Let's explore the key parameters:\n","\n","### Temperature\n","\n","Temperature controls randomness in the output. Lower values (near 0) make responses more deterministic and focused, while higher values (near 1-2) increase creativity and randomness.\n","\n","- **Low temperature (0.0-0.3)**: Good for factual Q&A, code generation, structured data extraction\n","- **Medium temperature (0.4-0.7)**: Balanced for general conversation and content creation\n","- **High temperature (0.8-1.0+)**: Creative writing, brainstorming, divergent thinking"]},{"cell_type":"code","execution_count":null,"id":"ac24b19c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ac24b19c","executionInfo":{"status":"ok","timestamp":1752775602930,"user_tz":240,"elapsed":3221,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"ab8c598a-8dc1-468f-9ea7-26042c45c56b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Temperature 0.0:\n","\"Empower your drive with Volt - the future of electric mobility.\"\n","--------------------------------------------------\n","\n","Temperature 0.5:\n","\"Powering the future with Volt: Electrify your drive!\"\n","--------------------------------------------------\n","\n","Temperature 1.0:\n","\"Experience the power and efficiency of Volt: driving into the future.\"\n","--------------------------------------------------\n","\n","Temperature 1.5:\n","\"Unleash the power of Volt: Electric driving, redefined.\"\n","--------------------------------------------------\n"]}],"source":["def compare_temperatures(prompt: str, temperatures: List[float] = [0.0, 0.5, 1.0, 1.5]):\n","    \"\"\"Compare model outputs at different temperature settings.\"\"\"\n","    results = {}\n","\n","    for temp in temperatures:\n","        response = client.chat.completions.create(\n","            model=\"gpt-3.5-turbo\",\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            temperature=temp,\n","            # seed=42  # Using seed for better comparison\n","        )\n","        results[f\"Temperature {temp}\"] = response.choices[0].message.content\n","\n","    return results\n","\n","# Example\n","creative_prompt = \"Generate a slogan for a new electric car brand called 'Volt'\"\n","temp_comparison = compare_temperatures(creative_prompt)\n","\n","for temp, response in temp_comparison.items():\n","    print(f\"\\n{temp}:\")\n","    print(f\"{response}\\n{'-' * 50}\")"]},{"cell_type":"markdown","id":"1e4429a5","metadata":{"cell_marker":"\"\"\"","lines_to_next_cell":1,"id":"1e4429a5"},"source":["### Top-p Sampling (Nucleus Sampling)\n","\n","Controls token diversity by considering only the tokens that comprise the top_p probability mass.\n","\n","- **Low top_p (0.1-0.4)**: More focused and deterministic responses\n","- **Medium top_p (0.5-0.7)**: Balanced, somewhat diverse responses\n","- **High top_p (0.8-1.0)**: More varied, potentially more creative responses\n","\n","### Max Tokens\n","\n","Limits the maximum length of the generated response. Important for:\n","- Controlling response length for specific formats\n","- Managing costs\n","- Preventing unnecessarily verbose outputs\n","\n","### Frequency and Presence Penalties\n","\n","Controls repetition in the model's output:\n","\n","- **Frequency penalty**: Reduces the likelihood of repeating tokens that have already appeared frequently\n","- **Presence penalty**: Reduces the likelihood of repeating any token that has appeared before, regardless of frequency\n","\n","Useful for long-form content to avoid circular reasoning or repetitive language."]},{"cell_type":"code","execution_count":null,"id":"6d54f61f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6d54f61f","executionInfo":{"status":"ok","timestamp":1752769488158,"user_tz":240,"elapsed":4433,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"43f68f85-4a42-462b-9b25-faefb4f94929"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","No penalty:\n","1. Improved physical health: Regular exercise can help prevent chronic diseases such as heart disease, diabetes, and obesity. It also helps strengthen muscles and bones, improve flexibility, and increase overall fitness levels.\n","\n","2. Mental health benefits: Exercise has been shown to have a positive impact on mental health by reducing symptoms of anxiety and depression, improving mood, and boosting self-esteem. It can also help alleviate stress and improve cognitive function.\n","\n","3. Weight management: Regular exercise can help with weight loss or weight maintenance by burning calories and increasing metabolism. It also helps build muscle mass, which can further contribute to a healthy body composition.\n","\n","4. Increased energy levels: Engaging in regular physical activity can help boost energy levels and combat feelings of fatigue. Exercise can improve\n","--------------------------------------------------\n","\n","Frequency penalty 1.0:\n","1. Improved physical health: Regular exercise can help reduce the risk of chronic diseases such as heart disease, diabetes, and obesity by improving cardiovascular health, boosting metabolism, and maintaining a healthy weight.\n","\n","2. Enhanced mental well-being: Exercise has been shown to reduce symptoms of depression, anxiety, and stress by releasing endorphins that act as natural mood lifters.\n","\n","3. Increased energy levels: Engaging in regular physical activity can boost energy levels and improve overall stamina and endurance throughout the day.\n","\n","4. Better sleep quality: Regular exercise can help regulate sleep patterns and improve overall sleep quality by promoting relaxation and reducing insomnia.\n","\n","5. Improved cognitive function: Exercise has been shown to enhance brain function, memory retention, and concentration by increasing blood flow to the\n","--------------------------------------------------\n","\n","Presence penalty 1.0:\n","1. Improved physical health: Regular exercise can help improve cardiovascular health, strengthen muscles and bones, and boost the immune system.\n","2. Weight management: Exercise can help control weight by burning calories and increasing metabolism.\n","3. Mental well-being: Physical activity releases endorphins, which can help reduce stress, anxiety, and depression.\n","4. Increased energy levels: Regular exercise can improve overall stamina and energy levels, making daily activities easier to perform.\n","5. Better sleep: Exercise can promote better sleep patterns and help individuals fall asleep faster and enjoy a more restful night's sleep.\n","--------------------------------------------------\n"]}],"source":["def compare_penalties(prompt: str):\n","    \"\"\"Compare model outputs with different frequency and presence penalties.\"\"\"\n","    results = {}\n","\n","    # No penalty\n","    response = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=[{\"role\": \"user\", \"content\": prompt}],\n","        temperature=0.7,\n","        max_tokens=150\n","    )\n","    results[\"No penalty\"] = response.choices[0].message.content\n","\n","    # Frequency penalty\n","    response = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=[{\"role\": \"user\", \"content\": prompt}],\n","        temperature=0.7,\n","        max_tokens=150,\n","        frequency_penalty=1.0\n","    )\n","    results[\"Frequency penalty 1.0\"] = response.choices[0].message.content\n","\n","    # Presence penalty\n","    response = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=[{\"role\": \"user\", \"content\": prompt}],\n","        temperature=0.7,\n","        max_tokens=150,\n","        presence_penalty=1.0\n","    )\n","    results[\"Presence penalty 1.0\"] = response.choices[0].message.content\n","\n","    return results\n","\n","# Example\n","prompt = \"List 5 benefits of regular exercise.\"\n","penalty_comparison = compare_penalties(prompt)\n","\n","for setting, response in penalty_comparison.items():\n","    print(f\"\\n{setting}:\")\n","    print(f\"{response}\\n{'-' * 50}\")"]},{"cell_type":"markdown","id":"4b27a823","metadata":{"cell_marker":"\"\"\"","lines_to_next_cell":1,"id":"4b27a823"},"source":["## 5 - Context Length Management\n","\n","The context length is the total number of tokens the model can process in a single request, including both the input (prompt) and output (completion). Managing context effectively is crucial for:\n","\n","1. **Cost optimization**: Fewer tokens = lower cost\n","2. **Response quality**: Most relevant context = better answers\n","3. **Performance**: Smaller contexts = faster responses\n","\n","### Token Counting\n","\n","Tokens are the basic units processed by LLMs - roughly 4 characters or 3/4 of a word in English:"]},{"cell_type":"code","execution_count":null,"id":"066e4615","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"066e4615","executionInfo":{"status":"ok","timestamp":1752775874335,"user_tz":240,"elapsed":1068,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"64ac41a7-f930-4908-bb4c-e195399fde30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Text 1 (11 characters):\n","- 2 tokens\n","- Approx. 0.73x character-to-token ratio\n","Text 2 (65 characters):\n","- 11 tokens\n","- Approx. 0.68x character-to-token ratio\n","Text 3 (259 characters):\n","- 48 tokens\n","- Approx. 0.74x character-to-token ratio\n"]}],"source":["def count_tokens(text: str, model: str = \"gpt-3.5-turbo\"):\n","    \"\"\"Count the number of tokens in a text string for a specific model.\"\"\"\n","    encoding = tiktoken.encoding_for_model(model)\n","    tokens = encoding.encode(text)\n","    return len(tokens)\n","\n","# Examples\n","texts = [\n","    \"Hello world\",\n","    \"This is a slightly longer sentence to demonstrate token counting.\",\n","    \"\"\"This is a much longer paragraph that contains multiple sentences.\n","    Token counting becomes more important as the length of text increases,\n","    especially when working with models that have context limits and\n","    when you're paying per token for API usage.\"\"\"\n","]\n","\n","for i, text in enumerate(texts):\n","    print(f\"Text {i+1} ({len(text)} characters):\")\n","    print(f\"- {count_tokens(text)} tokens\")\n","    print(f\"- Approx. {count_tokens(text) / (len(text) / 4):.2f}x character-to-token ratio\")"]},{"cell_type":"markdown","id":"ce1719ad","metadata":{"cell_marker":"\"\"\"","lines_to_next_cell":1,"id":"ce1719ad"},"source":["### Context Window Considerations\n","\n","Different models have different context window sizes. For example:\n","\n","- GPT-3.5 Turbo: 16K tokens\n","- GPT-4: 8K-128K tokens (depending on version)\n","- Claude 3 Opus: 200K tokens\n","- Mistral Large: 32K tokens\n","\n","### Strategies for Managing Large Contexts\n","\n","1. **Chunking**: Break large documents into smaller segments\n","2. **Summarization**: Generate summaries of lengthy content\n","3. **Selective Context**: Only include the most relevant context\n","4. **Sliding Window**: Process long documents in overlapping chunks\n","\n","Let's implement a simple chunking strategy:"]},{"cell_type":"code","execution_count":null,"id":"3401b97c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3401b97c","executionInfo":{"status":"ok","timestamp":1752776106590,"user_tz":240,"elapsed":55,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"e3520053-512e-4fc7-8ad6-7d890affeaca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Split into 8 chunks with ~100 tokens each and 20 tokens overlap\n","\n","Chunk 1:\n","[The Massachusetts Institute of Technology (MIT) is a private research university located in Cambridge, Massachusetts, United States. It was founded in 1861 in response to the increasing industrialization of the United States and adopted a European polytechnic university model, emphasizing laboratory instruction in applied science and engineering. Classes officially began in 1865, after delays caused by the American Civil War.\n","MIT is organized into five schools: the School of Architecture and Planning; the School of Engineering; the School of Humanities\n","\n","Chunk 2:\n"," into five schools: the School of Architecture and Planning; the School of Engineering; the School of Humanities, Arts, and Social Sciences; the Sloan School of Management; and the School of Science. In 2022, MIT also launched the Schwarzman College of Computing to integrate computer science and AI across disciplines. The institute has approximately 4,600 undergraduate students and 7,300 graduate students. It maintains a low student-to-faculty ratio and is known for its rigorous academic programs.\n","MIT\n","\n","Notice the overlap between the chunks\n"]}],"source":["def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100, model: str = \"gpt-3.5-turbo\"):\n","    \"\"\"Split text into overlapping chunks of approximately chunk_size tokens.\"\"\"\n","    encoding = tiktoken.encoding_for_model(model)\n","    tokens = encoding.encode(text)\n","\n","    chunks = []\n","    i = 0\n","    while i < len(tokens):\n","        # Get chunk of tokens\n","        chunk_end = min(i + chunk_size, len(tokens))\n","        chunk_tokens = tokens[i:chunk_end]\n","\n","        # Convert tokens back to text\n","        chunk_text = encoding.decode(chunk_tokens)\n","        chunks.append(chunk_text)\n","\n","        # Move to next chunk with overlap\n","        i += (chunk_size - overlap)\n","\n","    return chunks\n","\n","# Example with a longer text\n","long_text = \"\"\"[The Massachusetts Institute of Technology (MIT) is a private research university located in Cambridge, Massachusetts, United States. It was founded in 1861 in response to the increasing industrialization of the United States and adopted a European polytechnic university model, emphasizing laboratory instruction in applied science and engineering. Classes officially began in 1865, after delays caused by the American Civil War.\n","MIT is organized into five schools: the School of Architecture and Planning; the School of Engineering; the School of Humanities, Arts, and Social Sciences; the Sloan School of Management; and the School of Science. In 2022, MIT also launched the Schwarzman College of Computing to integrate computer science and AI across disciplines. The institute has approximately 4,600 undergraduate students and 7,300 graduate students. It maintains a low student-to-faculty ratio and is known for its rigorous academic programs.\n","MIT has long been a global leader in science, technology, and engineering education and research. Its faculty and alumni have been associated with numerous groundbreaking inventions and innovations, including radar, the digital computer, synthetic self-replicating molecules, and the development of the internet‚Äôs early architecture. As of 2024, MIT affiliates have won 100+ Nobel Prizes, 50+ National Medals of Science, and numerous other honors. The Institute places strong emphasis on entrepreneurship; companies founded by MIT alumni generate trillions of dollars in annual revenue and employ millions of people worldwide.\n","Research at MIT is supported by more than 30 departments and over 80 interdisciplinary labs and centers. Notable among them are the MIT Media Lab, the Computer Science and Artificial Intelligence Laboratory (CSAIL), the Lincoln Laboratory (a federally funded R&D center), and the Koch Institute for Integrative Cancer Research. MIT consistently ranks among the top universities in global research output and innovation metrics.\n","MIT's campus spans 168 acres along the Charles River and features a mix of historic and modern architecture, including buildings designed by noted architects such as Alvar Aalto, Frank Gehry, and I.M. Pei. Student life at MIT is known for being intense but collaborative, with a wide array of extracurricular opportunities, including athletics, music, entrepreneurship clubs, and the Independent Activities Period (IAP), a January term where students can pursue short courses, projects, or travel.\n","Admissions to MIT are highly competitive. For undergraduate admission, MIT does not consider legacy status or demonstrated interest and has a need-blind admissions policy for all applicants, including international students. Financial aid is entirely need-based, and MIT guarantees to meet the full demonstrated financial need of admitted students. In 2023, over 70% of undergraduates received financial aid, and the median annual debt at graduation was zero.\n","MIT‚Äôs motto, ‚ÄúMens et Manus‚Äù ‚Äî Latin for ‚ÄúMind and Hand‚Äù ‚Äî reflects its educational philosophy of integrating theoretical knowledge with practical application. The Institute remains at the forefront of addressing global challenges through education, research, and innovation, in fields ranging from climate change and clean energy to AI, biotechnology, and quantum computing.]\"\"\"\n","\n","chunks = chunk_text(long_text, chunk_size=100, overlap=20)\n","print(f\"Split into {len(chunks)} chunks with ~100 tokens each and 20 tokens overlap\")\n","\n","# Print the first two chunks\n","print(\"\\nChunk 1:\")\n","print(f\"{chunks[0]}\")\n","print(\"\\nChunk 2:\")\n","print(f\"{chunks[1]}\")\n","print(\"\\nNotice the overlap between the chunks\")\n"]},{"cell_type":"markdown","id":"6af2c40f","metadata":{"cell_marker":"\"\"\"","lines_to_next_cell":1,"id":"6af2c40f"},"source":["## 6 - Caching and Cost Optimization\n","\n","API calls to LLMs can be expensive. Implementing caching helps reduce costs and improve application performance.\n","\n","### Simple In-Memory Cache"]},{"cell_type":"code","execution_count":null,"id":"c2621a75","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c2621a75","executionInfo":{"status":"ok","timestamp":1752776195686,"user_tz":240,"elapsed":584,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"24c76111-14b8-4dbf-caaa-a55c1d5457a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["First call:\n","Cache miss, calling API... üåê\n","Response: The capital of France is Paris....\n","\n","Second call (should be cached):\n","Cache hit! ‚ö°Ô∏è\n","Response: The capital of France is Paris....\n"]}],"source":["class LLMCache:\n","    \"\"\"Simple in-memory cache for LLM responses.\"\"\"\n","\n","    def __init__(self, max_size: int = 1000):\n","        self.cache = {}\n","        self.max_size = max_size\n","\n","    def get_key(self, model: str, messages: List[Dict[str, str]], temperature: float):\n","        \"\"\"Create a cache key from request parameters.\"\"\"\n","        # Convert messages to a hashable form\n","        msg_str = json.dumps(messages, sort_keys=True)\n","        return f\"{model}|{msg_str}|{temperature}\"\n","\n","    def get(self, model: str, messages: List[Dict[str, str]], temperature: float):\n","        \"\"\"Retrieve a cached response if it exists.\"\"\"\n","        key = self.get_key(model, messages, temperature)\n","        return self.cache.get(key)\n","\n","    def set(self, model: str, messages: List[Dict[str, str]], temperature: float, response: str):\n","        \"\"\"Cache a response.\"\"\"\n","        if len(self.cache) >= self.max_size:\n","            # Simple eviction strategy: remove a random item\n","            self.cache.pop(next(iter(self.cache)))\n","\n","        key = self.get_key(model, messages, temperature)\n","        self.cache[key] = response\n","\n","# Cached completion function\n","cache = LLMCache()\n","\n","def cached_completion(messages: List[Dict[str, str]], model: str = \"gpt-3.5-turbo\", temperature: float = 0.7):\n","    \"\"\"Completion function with caching.\"\"\"\n","    # Check cache first\n","    cached_response = cache.get(model, messages, temperature)\n","    if cached_response:\n","        print(\"Cache hit! ‚ö°Ô∏è\")\n","        return cached_response\n","\n","    print(\"Cache miss, calling API... üåê\")\n","    # Make API call\n","    response = client.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","        temperature=temperature\n","    )\n","    result = response.choices[0].message.content\n","\n","    # Cache the result\n","    cache.set(model, messages, temperature, result)\n","\n","    return result\n","\n","# Example usage\n","print(\"First call:\")\n","response1 = cached_completion([{\"role\": \"user\", \"content\": \"What's the capital of France?\"}])\n","print(f\"Response: {response1[:50]}...\")\n","\n","print(\"\\nSecond call (should be cached):\")\n","response2 = cached_completion([{\"role\": \"user\", \"content\": \"What's the capital of France?\"}])\n","print(f\"Response: {response2[:50]}...\")"]},{"cell_type":"markdown","id":"2c0928db","metadata":{"cell_marker":"\"\"\"","id":"2c0928db"},"source":["### Production Caching Strategies\n","\n","For production applications, consider using:\n","\n","1. **Redis**: Fast, distributed in-memory cache\n","2. **DynamoDB/MongoDB**: For persistent caching across restarts\n","3. **Semantic caching**: Cache similar but not identical queries\n","4. **Time-based expiration**: Refresh cache after certain period\n","\n","### Cost Optimization Tips\n","\n","1. **Use the right model size**: Smaller models are cheaper and often sufficient\n","2. **Optimize prompts**: Clear, concise prompts reduce token usage\n","3. **Implement rate limiting**: Prevent accidental overuse\n","4. **Monitor usage**: Track consumption patterns for optimization\n","5. **Batch processing**: Process multiple requests at once when possible"]},{"cell_type":"markdown","id":"47633703","metadata":{"cell_marker":"\"\"\"","lines_to_next_cell":1,"id":"47633703"},"source":["## 7 - Function/Tool Calling\n","\n","One of the most powerful features of modern LLMs is their ability to use tools or functions to interact with external systems. This enables them to:\n","\n","1. Access real-time information\n","2. Perform calculations\n","3. Take actions in the real world\n","4. Generate structured outputs\n","\n","Let's implement a simple function calling example using OpenAI:"]},{"cell_type":"code","execution_count":null,"id":"362a1a7f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"362a1a7f","executionInfo":{"status":"ok","timestamp":1752776339749,"user_tz":240,"elapsed":4410,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"29d4024b-b451-4733-ed94-6829724241d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","User: What's the weather like in Tokyo?\n","Assistant: The weather in Tokyo is currently 8 degrees Celsius and sunny.\n","\n","User: What time is it in London?\n","Assistant: The current time in London is 18:18 (6:18 PM) on July 17, 2025.\n","\n","User: I'm planning a trip to Paris next week, should I pack a raincoat?\n","Assistant: It looks like the weather in Paris next week will be snowy with temperatures around 33¬∞C. I would recommend packing a warm coat or jacket rather than a raincoat to prepare for the snow. Safe travels!\n"]}],"source":["def get_weather(location: str, unit: str = \"celsius\"):\n","    \"\"\"Simulate getting weather information for a location.\"\"\"\n","    # In a real application, this would call a weather API\n","    import random\n","    temp = random.randint(0, 35)\n","    conditions = random.choice([\"sunny\", \"cloudy\", \"rainy\", \"snowy\"])\n","\n","    if unit == \"fahrenheit\":\n","        temp = temp * 9/5 + 32\n","\n","    return {\n","        \"location\": location,\n","        \"temperature\": temp,\n","        \"unit\": unit,\n","        \"conditions\": conditions\n","    }\n","\n","def get_current_time(timezone: str = \"UTC\"):\n","    \"\"\"Get the current time in a specific timezone.\"\"\"\n","    from datetime import datetime\n","    # Simplified implementation without actual timezone handling\n","    return {\"current_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"timezone\": timezone}\n","\n","# Define the function specifications for the LLM\n","tools = [\n","    {\n","        \"type\": \"function\",\n","        \"function\": {\n","            \"name\": \"get_weather\",\n","            \"description\": \"Get the current weather for a location\",\n","            \"parameters\": {\n","                \"type\": \"object\",\n","                \"properties\": {\n","                    \"location\": {\n","                        \"type\": \"string\",\n","                        \"description\": \"City name, e.g., 'San Francisco'\"\n","                    },\n","                    \"unit\": {\n","                        \"type\": \"string\",\n","                        \"enum\": [\"celsius\", \"fahrenheit\"],\n","                        \"description\": \"Temperature unit\"\n","                    }\n","                },\n","                \"required\": [\"location\"]\n","            }\n","        }\n","    },\n","    {\n","        \"type\": \"function\",\n","        \"function\": {\n","            \"name\": \"get_current_time\",\n","            \"description\": \"Get the current time in a specific timezone\",\n","            \"parameters\": {\n","                \"type\": \"object\",\n","                \"properties\": {\n","                    \"timezone\": {\n","                        \"type\": \"string\",\n","                        \"description\": \"Timezone, e.g., 'UTC', 'EST'\"\n","                    }\n","                }\n","            }\n","        }\n","    }\n","]\n","\n","# Function to handle the AI assistant with function calling\n","def assistant_with_tools(user_message: str):\n","    \"\"\"Process user messages and handle function calls as needed.\"\"\"\n","    messages = [{\"role\": \"user\", \"content\": user_message}]\n","\n","    # First, get the model's response with potential function calls\n","    response = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=messages,\n","        tools=tools,\n","        tool_choice=\"auto\"  # Let the model decide when to use functions\n","    )\n","\n","    response_message = response.choices[0].message\n","    messages.append(response_message)  # Add response to conversation history\n","\n","    # Check if the model wants to call a function\n","    if response_message.tool_calls:\n","        # Process each function call\n","        for tool_call in response_message.tool_calls:\n","            function_name = tool_call.function.name\n","            function_args = json.loads(tool_call.function.arguments)\n","\n","            # Execute the function\n","            function_response = None\n","            if function_name == \"get_weather\":\n","                function_response = get_weather(**function_args)\n","            elif function_name == \"get_current_time\":\n","                function_response = get_current_time(**function_args)\n","\n","            # Add function response to messages\n","            if function_response:\n","                messages.append({\n","                    \"role\": \"tool\",\n","                    \"tool_call_id\": tool_call.id,\n","                    \"name\": function_name,\n","                    \"content\": json.dumps(function_response)\n","                })\n","\n","        # Get a new response from the model after function call\n","        second_response = client.chat.completions.create(\n","            model=\"gpt-3.5-turbo\",\n","            messages=messages\n","        )\n","\n","        return second_response.choices[0].message.content\n","    else:\n","        # If no function was called, return the initial response\n","        return response_message.content\n","\n","# Test the assistant with function calling\n","queries = [\n","    \"What's the weather like in Tokyo?\",\n","    \"What time is it in London?\",\n","    \"I'm planning a trip to Paris next week, should I pack a raincoat?\"\n","]\n","\n","for query in queries:\n","    print(f\"\\nUser: {query}\")\n","    response = assistant_with_tools(query)\n","    print(f\"Assistant: {response}\")"]},{"cell_type":"markdown","id":"4bcf986f","metadata":{"cell_marker":"\"\"\"","lines_to_next_cell":1,"id":"4bcf986f"},"source":["## 8 - Practical Application: Simple Question-Answering System\n","\n","Let's put everything together to build a simple but effective question-answering system that demonstrates best practices:"]},{"cell_type":"code","execution_count":null,"id":"fd9fae50","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fd9fae50","executionInfo":{"status":"ok","timestamp":1752769497454,"user_tz":240,"elapsed":3899,"user":{"displayName":"Kevin Power","userId":"08364910535594493232"}},"outputId":"a21d61b2-cf7d-499f-8f5c-6ddac5765f89"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Q: What is the difference between supervised and unsupervised learning?\n","Calling gpt-3.5-turbo API...\n","API call completed in 1.13s\n","A: Supervised learning is a type of machine learning where the model is trained on labeled data, meaning it is given input data along with the correct output. The model learns to map input data to the correct output based on the labeled examples.\n","\n","Unsupervised learning, on the other hand, is a type of machine learning where the model is trained on unlabeled data. The model learns to find patterns and structures in the data without explicit guidance on the correct output.\n","\n","Q: Can you give me a simple example of each?\n","Calling gpt-3.5-turbo API...\n","API call completed in 1.58s\n","A: Sure!\n","\n","Supervised learning example: \n","Predicting housing prices based on features like size, location, and number of bedrooms. The model is trained on a dataset where each house's features are accompanied by its actual selling price.\n","\n","Unsupervised learning example:\n","Clustering customer data based on purchasing behavior to identify different customer segments. The model is trained on a dataset of customer transactions without any labels, and it groups customers based on similarities in their purchasing patterns.\n","\n","Q: What would be a good use case for reinforcement learning?\n","Calling gpt-3.5-turbo API...\n","API call completed in 1.15s\n","A: A good use case for reinforcement learning is in training AI agents to play games or optimize decision-making in complex environments. For example, training a reinforcement learning model to play chess, navigate a maze, or control a robot to perform tasks efficiently based on rewards and penalties received during the learning process.\n"]}],"source":["class SimpleQASystem:\n","    \"\"\"A simple question-answering system with caching and error handling.\"\"\"\n","\n","    def __init__(self, model=\"gpt-3.5-turbo\"):\n","        self.model = model\n","        self.client = OpenAI()\n","        self.cache = LLMCache()  # Reusing our cache from earlier\n","        self.conversation_history = []\n","        self.system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides concise, accurate answers.\"}\n","\n","    def count_tokens(self, text):\n","        \"\"\"Count tokens for the model.\"\"\"\n","        try:\n","            encoding = tiktoken.encoding_for_model(self.model)\n","            return len(encoding.encode(text))\n","        except Exception:\n","            # Fallback approximation\n","            return len(text.split()) * 1.3\n","\n","    def trim_conversation(self, max_tokens=3000):\n","        \"\"\"Trim conversation history to fit within token limit.\"\"\"\n","        # Always keep system message and last user question\n","        essential_msgs = [self.system_message, self.conversation_history[-1]]\n","        essential_tokens = sum(self.count_tokens(msg[\"content\"]) for msg in essential_msgs)\n","\n","        available_tokens = max_tokens - essential_tokens\n","        trimmed_history = []\n","\n","        # Add as many previous messages as will fit\n","        for msg in reversed(self.conversation_history[:-1]):\n","            msg_tokens = self.count_tokens(msg[\"content\"])\n","            if msg_tokens <= available_tokens:\n","                trimmed_history.insert(0, msg)\n","                available_tokens -= msg_tokens\n","            else:\n","                break\n","\n","        return [self.system_message] + trimmed_history + [self.conversation_history[-1]]\n","\n","    def answer_question(self, question, temperature=0.7):\n","        \"\"\"Answer a user question with caching and error handling.\"\"\"\n","        # Add question to history\n","        self.conversation_history.append({\"role\": \"user\", \"content\": question})\n","\n","        # Prepare messages with trimming if needed\n","        messages = self.trim_conversation()\n","\n","        # Check cache\n","        cache_key = (self.model, tuple([(m[\"role\"], m[\"content\"]) for m in messages]), temperature)\n","        cached = self.cache.get(self.model, messages, temperature)\n","        if cached:\n","            print(\"Using cached response\")\n","            answer = cached\n","        else:\n","            print(f\"Calling {self.model} API...\")\n","            try:\n","                # Make API call with error handling and timeout\n","                start_time = time.time()\n","                response = self.client.chat.completions.create(\n","                    model=self.model,\n","                    messages=messages,\n","                    temperature=temperature\n","                )\n","                answer = response.choices[0].message.content\n","                elapsed = time.time() - start_time\n","                print(f\"API call completed in {elapsed:.2f}s\")\n","\n","                # Cache the result\n","                self.cache.set(self.model, messages, temperature, answer)\n","\n","            except Exception as e:\n","                answer = f\"I'm sorry, I encountered an error: {str(e)}\"\n","                print(f\"API Error: {e}\")\n","\n","        # Add answer to conversation history\n","        self.conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n","        return answer\n","\n","    def reset_conversation(self):\n","        \"\"\"Reset the conversation history.\"\"\"\n","        self.conversation_history = []\n","\n","# Example usage\n","qa_system = SimpleQASystem()\n","\n","# Test with a few questions\n","questions = [\n","    \"What is the difference between supervised and unsupervised learning?\",\n","    \"Can you give me a simple example of each?\",\n","    \"What would be a good use case for reinforcement learning?\"\n","]\n","\n","for q in questions:\n","    print(f\"\\nQ: {q}\")\n","    answer = qa_system.answer_question(q)\n","    print(f\"A: {answer}\")"]},{"cell_type":"markdown","id":"c5757acf","metadata":{"cell_marker":"\"\"\"","lines_to_next_cell":2,"id":"c5757acf"},"source":["## 10 - Conclusion and Best Practices\n","\n","Congratulations! You've learned the essential skills for integrating LLMs into your applications via APIs. Here's a summary of best practices to remember:\n","\n","### Technical Best Practices\n","\n","1. **Use the right model for the task**: Balance cost vs. capability\n","2. **Set appropriate parameters**: Adjust temperature based on the task's creative needs\n","3. **Implement caching**: Reduce costs and improve responsiveness\n","4. **Manage context efficiently**: Be mindful of token limits\n","5. **Handle errors gracefully**: Always implement timeouts and fallbacks\n","6. **Use structured outputs when possible**: Function calling or JSON mode for predictable formats\n","\n","### Ethical and Security Considerations\n","\n","1. **Never expose API keys**: Use environment variables or secure vaults\n","2. **Monitor usage and costs**: Set up billing alerts and rate limits\n","3. **Implement content filtering**: Consider using moderation endpoints\n","4. **Provide clear user guidance**: Set expectations about AI-generated content\n","5. **Consider privacy implications**: Be careful with user data sent to APIs\n","\n","### Next Steps\n","\n","Now that you understand the basics of LLM API integration, you're ready to explore more advanced concepts like:\n","\n","1. Building sophisticated AI agents that can take actions\n","2. Implementing RAG (Retrieval Augmented Generation) systems\n","3. Fine-tuning models for your specific use cases\n","4. Developing hybrid systems that combine rule-based and ML approaches\n","\n","The companion notebook \"Agentic AI Lab\" explores how to build an AI agent that can make strategic decisions in a simulated environment, applying many of the concepts you've learned here."]}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}